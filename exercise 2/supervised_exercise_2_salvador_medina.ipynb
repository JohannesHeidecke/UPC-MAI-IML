{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Our first classifier.\n",
    "\n",
    "\n",
    "## I. GOAL OF THE EXERCISE\n",
    "\n",
    "In this exercise you will practice the basic pipeline of the supervised learning task. Implement a simple classifier. And will try to solve several hinderances found in the process.\n",
    "\n",
    "## II. DELIVERABLES\n",
    "As you progress in this exercise, you will find several questions you are expected to answer them properly with adequate figures when required and deliver the notebook with the working code used for generating and discussing the results in due time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\" style=\"border-radius:10px\"> **IMPORTANT:** Write in the next cell the name of the people that answer this notebook\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. OUR FIST CLASSIFIER.\n",
    "We are given the data in diabetes.mat and our goal is to predict the whether a person suffers from diabetes or not given her medical record. Our first model to try is linear regression as explained in ”A gentle introduction to supervised learning”.\n",
    "\n",
    "### A. Understanding and preprocessing our problem.\n",
    "The first step in the learning pipeline is to have a general picture of your dataset particularities.\n",
    "\n",
    "### B. Data set analysis\n",
    "Load the dataset and describe the basic properties of the data,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**QUESTION BLOCK 1:**\n",
    "<ol>\n",
    "<li> Which is the cardinality (number of examples) of the training set?</li> \n",
    "<li> Which is the dimensionality of the training set? </li>\n",
    "<li> Which is the mean value of the training set? </li>\n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "data = sio.loadmat('diabetes.mat')\n",
    "\n",
    "X = data['x'].T\n",
    "y = data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of examples is 768\n",
      "The number of dimensions of the training set is 8 \n",
      "\n",
      "The mean value for each dimension of the training set is:\n",
      "[   4.49467275  121.68676278   72.40518417   29.15341959  155.54822335\n",
      "   32.45746367    0.4718763    33.24088542]\n",
      "The mean value of the labels of training set is:  -0.302083333333\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "print \"The number of examples is\", X.shape[0]\n",
    "print \"The number of dimensions of the training set is\", X.shape[1],\"\\n\"\n",
    "\n",
    "print \"The mean value for each dimension of the training set is:\\n\",np.nanmean(X, axis=0)\n",
    "print \"The mean value of the labels of training set is: \", np.mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are some missing values with value NaN and som\n",
    "e categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**QUESTION BLOCK 2:**\n",
    "<ol>\n",
    "<li> Create a new dataset D1, replacing the NaN values with the mean value of the corresponding attribute without considering the missing values. </li>\n",
    "<li> Create a new dataset D2, replacing the NaN values with the mean value of the corresponding attribute without considering the missing values conditioned to the class they belong, i.e. replace the missing attribute values of class +1 with the mean of that attribute of the examples of class +1, and the same for the other class. </li>\n",
    "<li> **[Optional :]** Explain another method to deal with missing values and apply it to preprocess the training data. Include the reference of the method used. Consider this new dataset as D3. </li>\n",
    "<li> Which are the new mean values of each dataset?</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For D1, the mean we get is: [   4.49467275  121.68676278   72.40518417   29.15341959  155.54822335\n",
      "   32.45746367    0.4718763    33.24088542]\n",
      "For D2, the mean we get is: [   4.49265212  121.69735767   72.42814101   29.24704236  157.00352686\n",
      "   32.44642005    0.4718763    33.24088542]\n",
      "For D2 using median, the mean we get is: [   4.37760417  121.67708333   72.38932292   29.08984375  141.75390625\n",
      "   32.43463542    0.4718763    33.24088542]\n",
      "For D3 (Nearest), the mean we get is: [   4.41643663  121.63463542   72.40717593   29.10808412  152.62999553\n",
      "   32.40554687    0.4718763    33.24088542]\n"
     ]
    }
   ],
   "source": [
    "# 2 - 1\n",
    "def getD1(X,y,mean=None):\n",
    "    if mean is None:\n",
    "        # We have to compute it\n",
    "        mean = np.nanmean(X, axis=0)\n",
    "    D1=np.array(X)\n",
    "    for i in range(X.shape[1]):\n",
    "        D1[:,i][np.isnan(D1[:,i])]=mean[i]\n",
    "    return D1\n",
    "# 2 - 2\n",
    "def getD2(X,y):\n",
    "    D2=np.array(X)\n",
    "    for i in range(X.shape[1]):\n",
    "        # We are using references so that we dont have to copy anything\n",
    "        D2Col = np.asmatrix(D2[:,i]).T\n",
    "        meanC1 = np.nanmean(D2Col[y==1])\n",
    "        meanC2 = np.nanmean(D2Col[y==-1])\n",
    "        D2Col[np.logical_and(np.isnan(D2Col), y == 1)] = meanC1\n",
    "        D2Col[np.logical_and(np.isnan(D2Col), y == -1)] = meanC2\n",
    "    return D2\n",
    "\n",
    "# 2 - 3\n",
    "# We can, for example use the median or the most frequent value instead of the average\n",
    "# Another option is to use something similar to KNN, we get the closest examples and use the mean value\n",
    "# of the closest examples ()\n",
    "\n",
    "from collections import Counter\n",
    "def getD2Ext(X, y, strategy=\"mean\"):\n",
    "    D2=np.array(X)\n",
    "    for i in range(X.shape[1]):\n",
    "        # We are using references so that we dont have to copy anything\n",
    "        D2Col = np.asmatrix(D2[:,i]).T\n",
    "        estC1=0\n",
    "        estC2=0\n",
    "        if (strategy == \"mean\"):\n",
    "            estC1 = np.nanmean(D2Col[y==1])\n",
    "            estC2 = np.nanmean(D2Col[y==-1])\n",
    "        elif (strategy == \"median\"):\n",
    "            estC1 = np.nanmedian(np.asarray(D2Col[y==1]))\n",
    "            estC2 = np.nanmedian(np.asarray(D2Col[y==-1]))\n",
    "        elif (strategy == \"most_frequent\"):\n",
    "            estC1 = Counter(np.asarray(D2Col[y==1]).flatten()).most_common(1)[0][1]\n",
    "            estC2 = Counter(np.asarray(D2Col[y==-1]).flatten()).most_common(1)[0][1]\n",
    "        else:\n",
    "            raise Exception('Invalid strategy = '+strategy)\n",
    "        D2Col[np.logical_and(np.isnan(D2Col), y == 1)] = estC1\n",
    "        D2Col[np.logical_and(np.isnan(D2Col), y == -1)] = estC2\n",
    "    return D2\n",
    "\n",
    "from scipy import spatial\n",
    "def getD3(X,y,K=5,minDistanceDifferentClass=10000):\n",
    "    D3=np.array(X)\n",
    "    # In order to have an euclidean space, we have to find neighbours into an\n",
    "    # space with no nans, this is why we need to use one of the previously defined\n",
    "    # nan removal algorithms.\n",
    "    # This could be avoided using other mechanisms, but its implementation would take\n",
    "    # longer, so we exclude it from this exercise\n",
    "    Xnn = getD2Ext(X,y)\n",
    "    if minDistanceDifferentClass > 0:\n",
    "        # We add a last coordinate so that examples from different classes will very faraway\n",
    "        Xnn = np.concatenate((Xnn, (minDistanceDifferentClass/2)*y), axis=1)\n",
    "    for j in range (X.shape[1]):\n",
    "        # We get the matrix removing the target column\n",
    "        auxXnn = Xnn[:,[x for x in range(Xnn.shape[1]) if x != j]]\n",
    "        Tree = spatial.cKDTree(auxXnn, leafsize=100)\n",
    "        for i in range(X.shape[0]):\n",
    "            if np.isnan(D3[i,j]):\n",
    "                # Has a NAN value, we have to find the nearest\n",
    "                vnn=auxXnn[i]\n",
    "                # We have to add one to K, because the nearest neighbour is always the actual point\n",
    "                # and it is always nan, so it won't affect the result\n",
    "                nearest = Tree.query(vnn, k=(K+1))\n",
    "                meanNearest = np.nanmean(D3[nearest[1], j])\n",
    "                if np.isnan(meanNearest):\n",
    "                    # If all nearest neighbours are NaN, we can use the average calculated with\n",
    "                    # the other algorithms\n",
    "                    D3[i,j]=Xnn[i,j]\n",
    "                else:\n",
    "                    D3[i,j]=meanNearest\n",
    "    return D3\n",
    "\n",
    "# 2 - 4\n",
    "\n",
    "D1=getD1(X,y)\n",
    "D2=getD2(X,y)\n",
    "D2Ext=getD2Ext(X,y, strategy=\"median\")\n",
    "D3=getD3(X,y)\n",
    "\n",
    "\n",
    "meanD1 = np.mean(D1, axis=0)\n",
    "meanD2 = np.mean(D2, axis=0)\n",
    "meanD2Ext = np.mean(D2Ext, axis=0)\n",
    "meanD3 = np.mean(D3, axis=0)\n",
    "\n",
    "print 'For D1, the mean we get is:', meanD1\n",
    "print 'For D2, the mean we get is:', meanD2\n",
    "print 'For D2 using median, the mean we get is:', meanD2Ext\n",
    "print 'For D3 (Nearest), the mean we get is:', meanD3      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. A simple classifier\n",
    "\n",
    "Our first classifier is a thresholded regressor. Use and/or modify any of the methods you implemented for regression and apply it to find a linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**QUESTION BLOCK 3:**\n",
    "<ol>\n",
    "<li>In this model you have to learn the threshold value. Explain how you can accommodate this parameter.</li>\n",
    "<li>Report the normal vector of the separating hyperplane for each data set D1, D2, D3.</li>\n",
    "<li>Compute the error rates achieved on the training data. Are there significant differences? Report the method used and their parameters.</li>\n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For D1:\n",
      "The normal vector is:  [[  4.54830899e-02]\n",
      " [  1.28752133e-02]\n",
      " [ -2.76153057e-03]\n",
      " [  3.09178172e-04]\n",
      " [ -1.75410884e-04]\n",
      " [  2.73791130e-02]\n",
      " [  2.50953772e-01]\n",
      " [  4.98452234e-03]]\n",
      "Accuracy: 0.778645833333 (Error Rate: 22.1354166667 %)\n",
      "Precission (PPV):  0.73786407767\n",
      "Recall (TPR):  0.567164179104\n",
      "NPV:  0.79359430605\n",
      "TNR:  0.892\n",
      "\n",
      "\n",
      "For D2:\n",
      "The normal vector is:  [[ 0.0556913 ]\n",
      " [ 0.00988663]\n",
      " [-0.00151083]\n",
      " [ 0.01041739]\n",
      " [ 0.00186537]\n",
      " [ 0.0160551 ]\n",
      " [ 0.22947897]\n",
      " [ 0.00183327]]\n",
      "Accuracy: 0.798177083333 (Error Rate: 20.1822916667 %)\n",
      "Precission (PPV):  0.755656108597\n",
      "Recall (TPR):  0.623134328358\n",
      "NPV:  0.815356489945\n",
      "TNR:  0.892\n",
      "\n",
      "\n",
      "For D3:\n",
      "The normal vector is:  [[ 0.05251374]\n",
      " [ 0.01151445]\n",
      " [-0.00310255]\n",
      " [ 0.00428877]\n",
      " [ 0.00061375]\n",
      " [ 0.02273705]\n",
      " [ 0.23253102]\n",
      " [ 0.0029058 ]]\n",
      "Accuracy: 0.772135416667 (Error Rate: 22.7864583333 %)\n",
      "Precission (PPV):  0.718309859155\n",
      "Recall (TPR):  0.570895522388\n",
      "NPV:  0.792792792793\n",
      "TNR:  0.88\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This classifier consists in a function that maps each point to a certain value and a threshold\n",
    "# value. All points above that threshold are considered as elements of one class and the ones\n",
    "# bellow are considered as elements of the other.\n",
    "# This is:\n",
    "# For each point X, if sum(wi*xi) [i=1 to d] > threshold, X is of class 1. Otherwise X is of class -1.\n",
    "\n",
    "# In order to apply a regressor to this equation, if consider that all examples of class 1 have\n",
    "# a value of SCALE and the ones of class -1 -SCALE, we can clearly find the two components:\n",
    "# The weights of a linear regessor are: W=wn, ...,w1,w0\n",
    "# And as we said, the regressor has the form h(x) =  sign( sum(wi*xi) [i=0 to d] - threshold )\n",
    "# If we use the regressor weights in the previous function: h(x) = sign(W·X - (treshold+w0))\n",
    "# As such, we see that if we change the regressor's w0 to w0'=(w0+threshold), by changing w0'\n",
    "# we can change the threshold of the regressor.\n",
    "\n",
    "# This is the analytic regression solver copied from the first practice\n",
    "# We try to minimize the error of the regression as if the expected output was a continous function\n",
    "# For this reason, in this case the MSE error will always be greater than 0\n",
    "def analyticSolve(x, y, threshold=0, scale=1):\n",
    "    y = np.dot(scale,y)\n",
    "    w = np.zeros((x.shape[1]+1,1))\n",
    "    xWave = np.concatenate((x, np.ones((x.shape[0],1))), axis=1)\n",
    "    w = np.linalg.inv(xWave.T.dot(xWave)).dot(xWave.T).dot(y)\n",
    "    if threshold != 0:\n",
    "        w[w.shape[1]-1,0] = w[w.shape[1]-1,0]+threshold\n",
    "    return w\n",
    "\n",
    "def computeErrors(x, y, w):\n",
    "    TP=0\n",
    "    FP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    xWave = np.concatenate((x, np.ones((x.shape[0],1))), axis=1)\n",
    "    for i in range(x.shape[0]):\n",
    "        calculated = np.dot(np.matrix(xWave[i]), w)\n",
    "        if calculated > 0:\n",
    "            if y[i] == 1:\n",
    "                TP=TP+1\n",
    "            else:\n",
    "                FP=FP+1\n",
    "        else:\n",
    "            if y[i] == 1:\n",
    "                FN=FN+1\n",
    "            else:\n",
    "                TN=TN+1\n",
    "    return [[TP, FN], [FP,TN]]\n",
    "\n",
    "def printResults(w, pt, showWeights=False, showNormal=False):\n",
    "    if showWeights:\n",
    "        # This defines the plane (wn*xn + ... + w1*x1 + w0 = 0)\n",
    "        print \"The obtained weights are:\",w\n",
    "    if showNormal:\n",
    "        # The last coordinate of the weights (w0), is defined from one of the points in the plane\n",
    "        # (w0 = -(wn*xn0 + ... + w1*x10)) where xn0,...,x10 is a point\n",
    "        # The rest of the coordinates (wn,...,w1) defines the normal vector\n",
    "        print \"The normal vector is: \", w[0:w.shape[0]-1] \n",
    "    \n",
    "    accuracy=(pt[0][0]+pt[1][1])/float(pt[0][0]+pt[0][1]+pt[1][0]+pt[1][1])\n",
    "    precission=0\n",
    "    recall=0\n",
    "    npv=0\n",
    "    tnr=0\n",
    "    if pt[0][0]+pt[1][0] > 0:\n",
    "        precission = pt[0][0]/float(pt[0][0]+pt[1][0])\n",
    "        recall =pt[0][0]/float(pt[0][0]+pt[0][1])\n",
    "    if pt[1][1]+pt[0][1] > 0:\n",
    "        npv=pt[1][1]/float(pt[1][1]+pt[0][1])\n",
    "    if pt[1][1]+pt[1][0] > 0:\n",
    "        tnr=pt[1][1]/float(pt[1][1]+pt[1][0])\n",
    "    print \"Accuracy:\", accuracy, \"(Error Rate:\",(1-accuracy)*100,\"%)\"\n",
    "    print \"Precission (PPV): \", precission;\n",
    "    print \"Recall (TPR): \", recall;\n",
    "    print \"NPV: \", npv\n",
    "    print \"TNR: \", tnr\n",
    "\n",
    "\n",
    "xWave = np.concatenate((D1, np.ones((D1.shape[0],1))), axis=1)\n",
    "w = analyticSolve(D1, y)\n",
    "table = computeErrors(D1,y,w)\n",
    "print \"For D1:\"\n",
    "printResults(w,table,showNormal=True);\n",
    "print \"\\n\"\n",
    "\n",
    "w = analyticSolve(D2, y)\n",
    "table = computeErrors(D2,y,w)\n",
    "print \"For D2:\"\n",
    "printResults(w,table, showNormal=True);\n",
    "print \"\\n\"\n",
    "\n",
    "w = analyticSolve(D3, y)\n",
    "table = computeErrors(D3,y,w)\n",
    "print \"For D3:\"\n",
    "printResults(w,table,showNormal=True);\n",
    "print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training error is a poor estimation of the generalization error. Let us test what happens in a test set created by holding-out a certain percentage of the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\">**QUESTION BLOCK 4:**\n",
    "\n",
    "Repeat the learning process in block 3 using just D2 but holding-out the last fifth of the data set for testing purposes, i.e. use the first 4/5-th for training and the last 1/5-th for testing. Follow exactly the following steps in your process:\n",
    "<ol>\n",
    "<li> Clear your workspace: `%reset -f` at the begining of the cell. </li>\n",
    "<li> Preprocess the data replacing the NaN using the method for creating D2.</li>\n",
    "<li> Split your data in two sets: the first 4/5-th is to be used for training and\n",
    "the last 1/5-th will be used for testing purposes. Use a random seed value equal to 42.</li>\n",
    "<li> Train your model on the training set.</li>\n",
    "<li> Answer the following questions: Which is the error rate on your training\n",
    "data? Which is the error rate on your test data? Are they similar? Did you expect that behavior? Why?</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "[[131, 79], [40, 364]]\n",
      "Accuracy: 0.806188925081 (Error Rate: 19.3811074919 %)\n",
      "Precission (PPV):  0.766081871345\n",
      "Recall (TPR):  0.62380952381\n",
      "NPV:  0.821670428894\n",
      "TNR:  0.90099009901\n",
      "-------------------------------\n",
      "Test:\n",
      "[[32, 26], [10, 86]]\n",
      "Accuracy: 0.766233766234 (Error Rate: 23.3766233766 %)\n",
      "Precission (PPV):  0.761904761905\n",
      "Recall (TPR):  0.551724137931\n",
      "NPV:  0.767857142857\n",
      "TNR:  0.895833333333\n"
     ]
    }
   ],
   "source": [
    "# 4 - 1\n",
    "# %reset -f\n",
    "\n",
    "# 4 - 2\n",
    "D2=getD2(X,y)\n",
    "D2y=np.array(y)\n",
    "\n",
    "# 4 - 3\n",
    "# Set the seed\n",
    "np.random.seed(42)\n",
    "# Shuffle the dataset\n",
    "indices = np.random.permutation(D2.shape[0])\n",
    "# Create the corpuses\n",
    "numTrainingExamples = int(0.8*D2.shape[0])\n",
    "TrD2x, TeD2x = D2[indices[:numTrainingExamples]], D2[indices[numTrainingExamples:]]\n",
    "TrD2y, TeD2y = D2y[indices[:numTrainingExamples]], D2y[indices[numTrainingExamples:]]\n",
    "\n",
    "# 4 - 4\n",
    "w = analyticSolve(TrD2x, TrD2y)\n",
    "\n",
    "# 4 - 5\n",
    "print \"Training:\"\n",
    "tableTraining = computeErrors(TrD2x,TrD2y,w)\n",
    "print tableTraining\n",
    "printResults(w,tableTraining);\n",
    "print \"-------------------------------\"\n",
    "print \"Test:\"\n",
    "tableTraining = computeErrors(TeD2x,TeD2y,w)\n",
    "print tableTraining\n",
    "printResults(w,tableTraining);\n",
    "\n",
    "# As we can see, there is a difference in the accuracy value obtained in the\n",
    "# training set and in the test set. This is to be expected as the classifier is optimized for the\n",
    "# examples in the training set, and hence the accuracy is better when checking that set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\"> **QUESTION BLOCK 5:**\n",
    "Repeat the process in block 4 changing the order of some of the steps. Follow exactly the following steps in your process:\n",
    "<ol>\n",
    "<li> Clear your workspace with `%reset -f`.</li>\n",
    "<li> Split your data in two sets: the first 4/5-th is to be used for training and the last 1/5-th will be used for testing purposes. Use random state or random seed value of 42. </li>\n",
    "<li> Preprocess the data replacing the NaN using the method for creating D2. But this time use only the data corresponding to the training set. </li>\n",
    "<li> Train your model on the training set.</li>\n",
    "<li> Replace the NaN values using the means computed on the training data. </li>\n",
    "<li> Answer the following questions: Which is the error rate on your training data? Which is the error rate on your test data? Are they similar? Did you expect that behavior? Why? </li>\n",
    "<li> Compare these results with the ones in block 4. Do we achieve better or worse results? Why?</li>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "Accuracy: 0.804560260586 (Error Rate: 19.5439739414 %)\n",
      "Precission (PPV):  0.761627906977\n",
      "Recall (TPR):  0.62380952381\n",
      "NPV:  0.821266968326\n",
      "TNR:  0.898514851485\n",
      "-------------------------------\n",
      "Test:\n",
      "Accuracy: 0.746753246753 (Error Rate: 25.3246753247 %)\n",
      "Precission (PPV):  0.74358974359\n",
      "Recall (TPR):  0.5\n",
      "NPV:  0.747826086957\n",
      "TNR:  0.895833333333\n"
     ]
    }
   ],
   "source": [
    "# I am defining this exercise as a block in order to be able to use it in the last exercise\n",
    "def block5(X,y, trainingRatio=0.8):\n",
    "    # 5 - 1\n",
    "    # %reset -f\n",
    "\n",
    "    # 5 - 2\n",
    "    # Set the seed\n",
    "    np.random.seed(42)\n",
    "    # Shuffle the dataset\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    # Create the corpuses\n",
    "    numTrainingExamples = int(trainingRatio*X.shape[0])\n",
    "    Trx, Tex = X[indices[:numTrainingExamples]], X[indices[numTrainingExamples:]]\n",
    "    Try, Tey = y[indices[:numTrainingExamples]], y[indices[numTrainingExamples:]]\n",
    "\n",
    "    # 5 - 3\n",
    "    TrD2x=getD2(Trx,Try)\n",
    "\n",
    "    # 5 - 4\n",
    "    w = analyticSolve(TrD2x, Try)\n",
    "\n",
    "    # 5 - 5\n",
    "    TrMean = np.nanmean(Trx, axis=0)\n",
    "    TeD1x = getD1(Tex,Tey,TrMean)\n",
    "    \n",
    "    return [w, TrD2x, Try, TeD1x, Tey]\n",
    "\n",
    "w, TrD2x, Try, TeD1x, Tey = block5(X,y);\n",
    "\n",
    "# 5 - 6\n",
    "print \"Training:\"\n",
    "TrTable = computeErrors(TrD2x,Try,w)\n",
    "printResults(w,TrTable);\n",
    "print \"-------------------------------\"\n",
    "print \"Test:\"\n",
    "TeTable = computeErrors(TeD1x,Tey,w)\n",
    "printResults(w,TeTable);\n",
    "\n",
    "# Just like in the previous exercise, there is a considerable difference between the two corpuses.\n",
    "# In this case it is even greater. This is because he mechanism to remove NaNs in the previous exercise \n",
    "# was done taking into consideration the values and points of the test set. This is certainly odd, because\n",
    "# in a real case scenario we cannot replace NaN values depending on the value that we want to infer (D2 uses the class).\n",
    "# For this reason, in the previous exercise the training set and test set were more similar and more correlated than\n",
    "# in this second exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\" style = \"border-radius:10px\"> **QUESTION BLOCK 6:**\n",
    "<ol>\n",
    "<li> Repeat the process in block 5 changing the percentage of the data for training and testing. Plot a graph with the training and test error rates for each splitting percentage point. Comment the results.</li>\n",
    "<li> Add to the plot the upper bound on the generalization error using the equation of the slides for VC dimension equal to $d + 1$. Discuss the result.</li>\n",
    "<li> How many samples does the bound predict in order to have 1% error deviation with a confidence of 95%? And with confidence 50%? What about 5% and 10% error deviation with 95% confidence? Comment the behavior according to your observations.</li>\n",
    "</ol>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xa34acf8>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAEPCAYAAABWXy0pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nb8QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9bb4400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TrErrors=[]\n",
    "TeErrors=[]\n",
    "TrPcts=[]\n",
    "for i in range(2,100,2):\n",
    "    w, TrD2x, Try, TeD1x, Tey = block5(X,y,i/float(100))\n",
    "    tableTraining = computeErrors(TrD2x,Try,w)\n",
    "    \n",
    "    # Calculate error in training\n",
    "    TrTable = computeErrors(TrD2x,Try,w)\n",
    "    TrError=(TrTable[0][1]+TrTable[1][0])/float(TrTable[0][0]+TrTable[0][1]+TrTable[1][0]+TrTable[1][1])\n",
    "    \n",
    "    # Calculate error in Test\n",
    "    TeTable = computeErrors(TeD1x,Tey,w)\n",
    "    TeError=(TeTable[0][1]+TeTable[1][0])/float(TeTable[0][0]+TeTable[0][1]+TeTable[1][0]+TeTable[1][1])\n",
    "    \n",
    "    # Add to plots\n",
    "    TrErrors.append(TrError)\n",
    "    TeErrors.append(TeError)\n",
    "    TrPcts.append(i)\n",
    "\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.ylabel('Error Rate')\n",
    "plt.xlabel('% of Training')\n",
    "plt.hold('on')\n",
    "plt.plot( TrPcts, TrErrors, label=\"Training\")\n",
    "plt.plot(TrPcts, TeErrors,  label=\"Test\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title='Corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I DON'T KNOW HOW TO APPLY THE FORMULA"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
